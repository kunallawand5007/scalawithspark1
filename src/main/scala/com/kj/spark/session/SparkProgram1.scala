package com.kj.spark.session

import org.apache.spark.ml.feature.VectorIndexer

object SparkProgram1 extends App  with Context {


 /* var rdd=sparkContextObj.textFile("D:\\lockdown\\spark-scala-ws\\ml-100k\\directory.txt")

  var newrdd= rdd.flatMap(s=>s.split(" ")).filter(s=>s.length>3).collect().toList

  println(Console.BOLD)
  println("Word with length >3 ")
  newrdd.foreach(println)

  println(Console.BOLD)
  println("Word + Count in Map")
  var split= rdd.flatMap(s=>s.split(" "))
  split.count()
  var map= split.filter(s=>s.nonEmpty).map(s=>(s.toString,s.toString.length)).collectAsMap()
  map.foreach(println)*/






}
